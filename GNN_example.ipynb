{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import to_hetero, GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wren1\\AppData\\Local\\Temp\\ipykernel_25648\\3799031966.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  s['Errors?'].fillna('N', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   User                    10000 non-null  int64  \n",
      " 1   Card                    10000 non-null  int64  \n",
      " 2   Year                    10000 non-null  int64  \n",
      " 3   Month                   10000 non-null  int64  \n",
      " 4   Day                     10000 non-null  int64  \n",
      " 5   Amount                  10000 non-null  float64\n",
      " 6   Use Chip                10000 non-null  int32  \n",
      " 7   Merchant Name           10000 non-null  int64  \n",
      " 8   Merchant City           10000 non-null  int32  \n",
      " 9   Merchant State          10000 non-null  object \n",
      " 10  MCC                     10000 non-null  int64  \n",
      " 11  Errors?                 10000 non-null  int32  \n",
      " 12  Is Fraud?               10000 non-null  int64  \n",
      " 13  index                   10000 non-null  int64  \n",
      " 14  Current Age             10000 non-null  int64  \n",
      " 15  Retirement Age          10000 non-null  int64  \n",
      " 16  Gender                  10000 non-null  int32  \n",
      " 17  State                   10000 non-null  int32  \n",
      " 18  Zipcode                 10000 non-null  int64  \n",
      " 19  Yearly Income - Person  10000 non-null  float64\n",
      " 20  Total Debt              10000 non-null  float64\n",
      " 21  FICO Score              10000 non-null  int64  \n",
      " 22  Num Credit Cards        10000 non-null  int64  \n",
      " 23  Hour                    10000 non-null  int32  \n",
      " 24  Minute                  10000 non-null  int32  \n",
      " 25  Unknown State           10000 non-null  int64  \n",
      " 26  Transaction ID          10000 non-null  int64  \n",
      "dtypes: float64(3), int32(7), int64(16), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "#read in the credit card transaction data\n",
    "# The data is from IBM's credit card fraud detection dataset on Kaggle\n",
    "# https://www.kaggle.com/datasets/ealtman2019/credit-card-transactions.\n",
    "\n",
    "df = pd.read_csv('data/credit_card_transactions-ibm_v2.csv')\n",
    "\n",
    "# Take a small sample of the data for computational efficiency\n",
    "# To avoid extreme class imbalance, I will force 50% of the sample to be fraud\n",
    "\n",
    "s1 = df[df['Is Fraud?'] == 'No'].sample(n=5000, random_state=4)\n",
    "s2 = df[df['Is Fraud?'] == 'Yes'].sample(n=5000, random_state=4)\n",
    "\n",
    "s = pd.concat([s1, s2]).reset_index(drop=True)\n",
    "\n",
    "#add user information from user file to the data\n",
    "udf = pd.read_csv('data/sd254_users.csv')\n",
    "udf.reset_index(inplace=True)\n",
    "udf = udf[['index','Current Age','Retirement Age','Gender','State','Zipcode','Yearly Income - Person','Total Debt','FICO Score','Num Credit Cards']]\n",
    "s = pd.merge(s, udf, how='left', left_on='User',right_on='index')\n",
    "\n",
    "# Drop zip and state as they are missing values\n",
    "s.drop(columns=['Zip'], inplace=True)\n",
    "\n",
    "# Fill in value where no error\n",
    "s['Errors?'].fillna('N', inplace=True)\n",
    "\n",
    "# Convert Is Fraud to binary\n",
    "s['Is Fraud?'] = s['Is Fraud?'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Separate time into hours and minutes\n",
    "s['Hour'] = s['Time'].str.split(':').str[0].astype(int)\n",
    "s['Minute'] = s['Time'].str.split(':').str[1].astype(int)\n",
    "\n",
    "# Drop time\n",
    "s.drop(columns='Time', inplace=True)\n",
    "\n",
    "#convert dollars to floats\n",
    "for col in ['Yearly Income - Person', 'Total Debt', 'Amount']:\n",
    "    s[col] = s[col].str.replace('$', '').astype(float)\n",
    "\n",
    "# replace unknown merhcant states with unknown\n",
    "s['Merchant State']  = s['Merchant State'] .fillna('unknown')\n",
    "# create an indicator for unknown state\n",
    "s['Unknown State'] = s.apply(lambda row: 1 if row['Merchant State'] == 'unknown' else 0, axis=1)\n",
    "\n",
    "# Convert categorical to numeric with label encoder \n",
    "# (for simplicity using label encoding despite introduction of ordinality)\n",
    "le = LabelEncoder()\n",
    "s['Use Chip'] = le.fit_transform(s['Use Chip'])\n",
    "\n",
    "s['Merchant Name'] = le.fit_transform(s['Merchant Name'])\n",
    "\n",
    "s['Merchant City'] = le.fit_transform(s['Merchant City'])\n",
    "\n",
    "s['Errors?'] = le.fit_transform(s['Errors?'])\n",
    "\n",
    "s['User'] = le.fit_transform(s['User'])\n",
    "\n",
    "s['Gender'] = le.fit_transform(s['Gender'])\n",
    "\n",
    "s['State'] = le.fit_transform(s['State'])\n",
    "\n",
    "s['Zipcode'] = le.fit_transform(s['Zipcode'])\n",
    "\n",
    "# create unique id for transaction from 0 to n-1\n",
    "s['Transaction ID'] = range(0, len(s))\n",
    "s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "accuracy 0.764\n",
      "precision 0.8139866793529972\n",
      "recall 0.6844\n",
      "f1 score 0.7435897435897436\n"
     ]
    }
   ],
   "source": [
    "# 1. Logistic Regression for Predicting Fraud (baseline)\n",
    "\n",
    "# We will run a logistic regression to predict fraud as a baseline for comparison to the GNN model\n",
    "\n",
    "X = s.drop(columns=['Is Fraud?', 'User', 'Merchant Name','Transaction ID','Merchant State'])\n",
    "y = s['Is Fraud?']\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X,y)\n",
    "y_pred = lr.predict(X)\n",
    "\n",
    "print('Logistic Regression Results')\n",
    "print('accuracy',accuracy_score(y, y_pred))\n",
    "print('precision',precision_score(y, y_pred))\n",
    "print('recall',recall_score(y, y_pred))\n",
    "print('f1 score',f1_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user', 'merchant', 'transaction']\n",
      "[('user', 'makes', 'transaction'), ('transaction', 'reaches', 'merchant'), ('transaction', 'rev_makes', 'user'), ('merchant', 'rev_reaches', 'transaction')]\n",
      "HeteroData(\n",
      "  user={\n",
      "    num_nodes=1511,\n",
      "    x=[1511, 9],\n",
      "  },\n",
      "  merchant={\n",
      "    num_nodes=2010,\n",
      "    x=[2010, 1],\n",
      "  },\n",
      "  transaction={\n",
      "    num_nodes=10000,\n",
      "    x=[10000, 22],\n",
      "    y=[10000, 1],\n",
      "  },\n",
      "  (user, makes, transaction)={ edge_index=[2, 10000] },\n",
      "  (transaction, reaches, merchant)={ edge_index=[2, 10000] },\n",
      "  (transaction, rev_makes, user)={ edge_index=[2, 10000] },\n",
      "  (merchant, rev_reaches, transaction)={ edge_index=[2, 10000] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. GNN for Predicting Fraud\n",
    "\n",
    "# create a heterogenous graph\n",
    "data = HeteroData()\n",
    " \n",
    "# add nodes\n",
    "data['user'].num_nodes = s['User'].nunique()\n",
    "data['merchant'].num_nodes = s['Merchant Name'].nunique()\n",
    "data['transaction'].num_nodes = s['Transaction ID'].nunique()\n",
    "\n",
    "#create edges\n",
    "\n",
    "data['user','makes','transaction'].edge_index = torch.tensor(s[['User', 'Transaction ID']].values.T, dtype=torch.long)\n",
    "data['transaction','reaches','merchant'].edge_index = torch.tensor(s[['Transaction ID', 'Merchant Name']].values.T, dtype=torch.long)\n",
    "\n",
    "# add transaction features\n",
    "transaction_features = torch.tensor(s.drop(columns=['Is Fraud?', 'User', 'Merchant Name','Transaction ID','Merchant State']).values, dtype=torch.float)\n",
    "# print(transaction_features.shape)\n",
    "data['transaction'].x = transaction_features\n",
    "\n",
    "target = torch.tensor(s['Is Fraud?'].values, dtype=torch.float).unsqueeze(0).T\n",
    "# print(target.shape)\n",
    "data['transaction'].y = target\n",
    "\n",
    "#add user node features\n",
    "user_feats = s[['Current Age','Retirement Age','Gender','State','Zipcode','Yearly Income - Person','Total Debt','FICO Score','Num Credit Cards']].drop_duplicates().reset_index().drop(columns='index',axis=1)\n",
    "user_features = torch.tensor(user_feats.values,dtype=torch.float)\n",
    "# print(user_features.shape)\n",
    "data['user'].x = user_features\n",
    "\n",
    "#add merchant node features (just one for unknown states)\n",
    "x = s[['Merchant Name','Unknown State']].drop_duplicates()\n",
    "y = x.groupby('Merchant Name').prod()\n",
    "\n",
    "#merchant data is not 1x1 to merchant id, so created a column to represent if the merchant has no known states or if all states are known\n",
    "data['merchant'].x= torch.tensor(y['Unknown State'],dtype=torch.float).unsqueeze(0).T\n",
    "\n",
    "\n",
    "# Add reverse edges and normalize features\n",
    "data = T.ToUndirected()(data)\n",
    "data = T.NormalizeFeatures()(data)\n",
    "\n",
    "node_types, edge_types = data.metadata()\n",
    "\n",
    "print(node_types)\n",
    "\n",
    "print(edge_types)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.372481107711792\n",
      "0.7023478746414185\n",
      "0.5558483600616455\n",
      "0.5189362168312073\n",
      "0.536408543586731\n",
      "0.5142029523849487\n",
      "0.518513560295105\n",
      "0.5026587843894958\n",
      "0.4921687841415405\n",
      "0.4867297410964966\n",
      "0.48084062337875366\n",
      "0.4843391478061676\n",
      "0.5168926119804382\n",
      "0.506732702255249\n",
      "0.49973657727241516\n",
      "0.4980401396751404\n",
      "0.48850879073143005\n",
      "0.4860280752182007\n",
      "0.4808429181575775\n",
      "0.8215529322624207\n",
      "GNN Results\n",
      "accuracy 0.7901\n",
      "precision 0.7784603570742945\n",
      "recall 0.811\n",
      "f1 score 0.7943971005975121\n"
     ]
    }
   ],
   "source": [
    "# Define the GNN model with two layers of GraphConv layers\n",
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # -1 is lazy initialization of the shape, which will be inferred from the first forward pas\n",
    "        self.conv1 = GraphConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = GraphConv((-1, -1), out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GNN(hidden_channels=64, out_channels=2)\n",
    "\n",
    "# Convert the model to a heterogenous model\n",
    "# The model will now take a dictionary of node features and a dictionary of edge indices\n",
    "model = to_hetero(model, data.metadata(), aggr='sum')\n",
    "\n",
    " \n",
    "# Define the optimizer to be used in training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    " \n",
    "# Create dictionaries of node features and edge indices from graph data\n",
    "node_x_dict = {'user':data['user'].x, 'transaction': data['transaction'].x, 'merchant': data['merchant'].x}\n",
    "\n",
    "edge_ind_dict =  {('user', 'makes', 'transaction'): data['user', 'makes', 'transaction'].edge_index,\n",
    "                  ('transaction', 'reaches', 'merchant'): data['transaction', 'reaches', 'merchant'].edge_index,\n",
    "                  ('transaction', 'rev_makes', 'user'): data['transaction', 'rev_makes', 'user'].edge_index,\n",
    "                  ('merchant', 'rev_reaches', 'transaction'): data['merchant', 'rev_reaches', 'transaction'].edge_index\n",
    "                 }\n",
    "\n",
    " \n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(2000):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Perform a forward pass\n",
    "    out = model(node_x_dict, edge_ind_dict)\n",
    "    # Compute the loss based on the nodes in the 'transaction' type\n",
    "    actual = data['transaction'].y.squeeze().long()\n",
    "    loss = F.cross_entropy(out['transaction'], actual)\n",
    "    # Perform a backward pass\n",
    "    loss.backward()\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    # Print the loss at every 100th epoch\n",
    "    # This is useful for tracking convergence\n",
    "    if epoch % 100 == 0:\n",
    "        print(float(loss))\n",
    "\n",
    " \n",
    "# Evaluate the model on the training data\n",
    "model.eval()\n",
    "y_pred = model(node_x_dict, edge_ind_dict)\n",
    "y_pred = y_pred['transaction'].argmax(dim=1)\n",
    "\n",
    "y = data['transaction'].y.squeeze().long()\n",
    "\n",
    "print('GNN Results')\n",
    "print('accuracy',accuracy_score(y, y_pred))\n",
    "print('precision',precision_score(y, y_pred))\n",
    "print('recall',recall_score(y, y_pred))\n",
    "print('f1 score',f1_score(y, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
